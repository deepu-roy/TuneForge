# Fine-Tuning Pipeline Configuration Example
# Copy this file to config.env and customize for your needs
# Usage: ./train-and-convert.sh --config config.env

# ============================================
# Model Name
# ============================================
# Base name for your fine-tuned model
# This will be used consistently in all output paths
# Example: "my-coding-assistant", "rhyme-generator", "sql-expert"
MODEL_NAME="fine-tuned-model"

# ============================================
# Training Data
# ============================================
# Path to your training data in JSONL format
TRAIN_DATA="data/train.jsonl"

# ============================================
# Base Model
# ============================================
# Hugging Face model ID or local path
# Examples:
#   - TinyLlama/TinyLlama-1.1B-Chat-v1.0 (default)
#   - meta-llama/Llama-2-7b-hf
#   - mistralai/Mistral-7B-v0.1
BASE_MODEL="TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# ============================================
# Training Parameters
# ============================================
# Number of epochs to train
EPOCHS=3

# Learning rate (use smaller values for larger models)
LEARNING_RATE=2e-4

# Maximum sequence length (adjust based on your data and GPU memory)
MAX_SEQ_LENGTH=512

# ============================================
# GGUF Configuration
# ============================================
# GGUF quantization type (output precision)
# Common options:
#   - f32: Full 32-bit float (largest, best quality)
#   - f16: Half precision 16-bit float (recommended, good balance)
#   - q8_0: 8-bit quantization (smaller, slight quality loss)
#   - q4_0: 4-bit quantization (much smaller, noticeable quality loss)
#   - q4_1, q5_0, q5_1: Various 4-5 bit quantization schemes
# For full list, see llama.cpp documentation
GGUF_OUTTYPE="f16"

# ============================================
# Output Directories
# ============================================
# Where to save the LoRA adapter
ADAPTER_OUTPUT="outputs/adaptor/${MODEL_NAME}-mps"

# Where to save the merged model
MERGED_OUTPUT="outputs/merged/merged-${MODEL_NAME}"

# Where to save the GGUF file (uses MODEL_NAME and GGUF_OUTTYPE)
GGUF_OUTPUT="outputs/gguf/${MODEL_NAME}-${GGUF_OUTTYPE}.gguf"

# ============================================
# llama.cpp Configuration
# ============================================
# Path to your llama.cpp installation directory
# This is required for GGUF conversion
# Example: LLAMA_CPP_PATH="/Users/username/Code/llama.cpp"
# Leave empty to skip GGUF conversion
LLAMA_CPP_PATH=""

# GGUF quantization type (output precision)
# Common options:
#   - f32: Full 32-bit float (largest, best quality)
#   - f16: Half precision 16-bit float (recommended, good balance)
#   - q8_0: 8-bit quantization (smaller, slight quality loss)
#   - q4_0: 4-bit quantization (much smaller, noticeable quality loss)
#   - q4_1, q5_0, q5_1: Various 4-5 bit quantization schemes
# For full list, see llama.cpp documentation
GGUF_OUTTYPE="f16"

# ============================================
# Ollama Modelfile Configuration
# ============================================
# These settings control model behavior in Ollama
# The script will automatically generate a Modelfile with these parameters

# Temperature: Controls randomness (0.0 = deterministic, 2.0 = very random)
MODELFILE_TEMPERATURE=1

# Top P: Nucleus sampling threshold (0.0-1.0)
MODELFILE_TOP_P=0.9

# Repeat Penalty: Penalize repetition (1.0 = no penalty, higher = more penalty)
MODELFILE_REPEAT_PENALTY=1.05

# ============================================
# Pipeline Control
# ============================================
# Set to true to skip specific steps
# Useful for re-running only parts of the pipeline

# Skip training (use existing adapter)
SKIP_TRAINING=false

# Skip merging (use existing merged model)
SKIP_MERGE=false

# Skip GGUF conversion
SKIP_GGUF=false
