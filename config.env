# Fine-Tuning Pipeline Configuration
# Copy this file and customize for your needs
# Usage: ./train-and-convert.sh --config your-config.env

# ============================================
# Model Name
# ============================================
# Base name for your fine-tuned model (used in all output paths)
MODEL_NAME="fine-tuned-model"

# Training Data
TRAIN_DATA="data/train.jsonl"

# Base Model
BASE_MODEL="TinyLlama/TinyLlama-1.1B-Chat-v1.0"

# Training Parameters
EPOCHS=3
LEARNING_RATE=2e-4
MAX_SEQ_LENGTH=512

# GGUF Quantization Type
# Options: f32, f16, q8_0, q4_0, q4_1, q5_0, q5_1, etc.
# f16 is recommended for most use cases (good quality, reasonable size)
GGUF_OUTTYPE="f16"

# Output Directories/files
ADAPTER_OUTPUT="outputs/adaptor/${MODEL_NAME}-mps"
MERGED_OUTPUT="outputs/merged/merged-${MODEL_NAME}"
GGUF_OUTPUT="outputs/gguf/${MODEL_NAME}-${GGUF_OUTTYPE}.gguf"

# llama.cpp Path (required for GGUF conversion)
# Set this to your llama.cpp installation directory
# Example: LLAMA_CPP_PATH="/Users/username/Code/llama.cpp"
LLAMA_CPP_PATH=""

# Modelfile Configuration (for Ollama)
# These settings will be used to generate the Modelfile automatically
MODELFILE_TEMPERATURE=1
MODELFILE_TOP_P=0.9
MODELFILE_REPEAT_PENALTY=1.05

# Skip Steps (true/false)
# Set to true to skip specific pipeline steps
SKIP_TRAINING=false
SKIP_MERGE=false
SKIP_GGUF=false
